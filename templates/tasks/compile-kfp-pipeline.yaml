---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: compile-kfp-pipeline
spec:
  workspaces:
    - name: output
  params:
    - name: PIPELINE_NAME
      description: Name of the model
      type: string
    - name: CLUSTER_DOMAIN
      description: Cluster domain for kubeflow endpoint
      type: string
    - name: WORK_DIRECTORY
      description: Directory to work in
      type: string
  steps:
    - name: compile-pipeline
      workingDir: $(workspaces.output.path)/$(params.WORK_DIRECTORY)
      image: registry.redhat.io/ubi9/python-311@sha256:fc669a67a0ef9016c3376b2851050580b3519affd5ec645d629fd52d2a8b8e4a
      command: ["/bin/sh", "-c"]
      args:
        - |
          # Read version from file
          if [ -f "version" ]; then
            VERSION=$(cat version | tr -d '\n')
            echo "Read version from file: ${VERSION}"
          else
            echo "ERROR: version file not found"
            exit 1
          fi
          
          echo "Compiling KFP pipeline for model: $(params.PIPELINE_NAME) version: ${VERSION}"
          echo "Working directory: $(workspaces.output.path)/$(params.WORK_DIRECTORY)"
          ls -la
          
          # Install required dependencies
          python3 -m pip install kfp==2.9.0 kfp-kubernetes==1.3.0 pyyaml
          
          # Create dynamic compilation script
          cat > compile_pipeline.py << EOF
          import kfp
          import yaml
          import importlib
          import os
          
          # Load pipeline configuration
          config_file = "pipeline_config.yaml"
          if os.path.exists(config_file):
              with open(config_file, 'r') as f:
                  config = yaml.safe_load(f)
              print(f"Loaded configuration from {config_file}")
          else:
              # Fallback to default configuration
              config = {
                  "entrypoint": {
                      "module": "prod_train_save_pipeline",
                      "function": "training_pipeline"
                  },
                  "metadata": {
                      "hyperparameters": {"epochs": 2},
                      "model_name": "default-model",
                      "prod_flag": True
                  }
              }
              print(f"Using default configuration (no {config_file} found)")
          
          # Dynamic import of the pipeline function
          module_name = config["entrypoint"]["module"]
          function_name = config["entrypoint"]["function"]
          
          print(f"Importing {function_name} from {module_name}")
          module = importlib.import_module(module_name)
          pipeline_function = getattr(module, function_name)
          
          # Build metadata by merging config defaults with runtime values
          metadata = config.get("metadata", {}).copy()
          metadata.update({
              "version": "${VERSION}",
              "cluster_domain": "$(params.CLUSTER_DOMAIN)",
              "model_storage_pvc": "model-pvc"
          })
          
          print(f"Compiling pipeline with metadata: {metadata}")
          kfp.compiler.Compiler().compile(
              pipeline_function, 
              '$(params.PIPELINE_NAME).yaml', 
              pipeline_parameters=metadata
          )
          print("Pipeline compiled successfully to $(params.PIPELINE_NAME).yaml")
          EOF
          
          # Execute the compilation
          python3 compile_pipeline.py
          
          # Verify the compiled file exists
          if [ -f "$(params.PIPELINE_NAME).yaml" ]; then
            echo "Pipeline compilation successful"
            ls -la $(params.PIPELINE_NAME).yaml
          else
            echo "ERROR: Pipeline compilation failed - $(params.PIPELINE_NAME).yaml not found"
            exit 1
          fi